{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assignment 7\n",
    "\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paragraph.txt') as f:\n",
    "    paragraph = f.read()\n",
    "    paragraph = paragraph.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the first step when working with language tasks, it simplifies the input data by splitting it into sentences or words, as per the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentence_tokens = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sentence tokens :', len(sentence_tokens))\n",
    "print('Sentence tokens :', sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "word_tokens = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of word tokens :', len(word_tokens))\n",
    "print('Word tokens :', word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words :', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word_token for word_token in word_tokens if word_token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtered word tokens :', word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-abverb, eg- where, when\n",
    "'''\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('POS Tagged form of filtered word tokens :')\n",
    "for tag in tagged:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results of Stemming')\n",
    "stemmed = {word: ps.stem(word) for word in word_tokens}\n",
    "for pair in stemmed.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results of Lemmatization')\n",
    "lemmatized = {word: lemmatizer.lemmatize(word) for word in word_tokens}\n",
    "for pair in lemmatized.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_convert_1d(arr):\n",
    "    arr = np.array(arr)\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = []\n",
    "def cosine(trans):\n",
    "    cos.append(cosine_similarity(trans[0], trans[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhatten = []\n",
    "def manhatten_distance(trans):\n",
    "    manhatten.append(pairwise_distances(trans[0], trans[1], metric = 'manhattan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = []\n",
    "def euclidean_function(vectors):\n",
    "    euc=euclidean_distances(vectors[0], vectors[1])\n",
    "    euclidean.append(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(str1, str2):\n",
    "    vect = TfidfVectorizer()\n",
    "    vect.fit(word_tokens)\n",
    "    corpus = [str1,str2]\n",
    "    trans = vect.transform(corpus)\n",
    "    euclidean_function(trans)\n",
    "    cosine(trans)\n",
    "    manhatten_distance(trans)\n",
    "    return convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert():\n",
    "    dataf = pd.DataFrame()\n",
    "    lis2 = arr_convert_1d(manhatten)\n",
    "    dataf['manhattan'] = lis2\n",
    "    lis2 = arr_convert_1d(cos)\n",
    "    dataf['cos_sim'] = lis2\n",
    "    lis2 = arr_convert_1d(euclidean)\n",
    "    dataf['euclidean'] = lis2\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'rsfhcui'\n",
    "str2 = 'ukjiorgd'\n",
    "newData = tfidf(str1,str2);\n",
    "print(newData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
